{"cells":[{"cell_type":"markdown","id":"340687b6-a1cc-4be6-9d79-baca3d7e19d0","metadata":{},"outputs":[],"source":["<p style=\"text-align:center\">\n","    <a href=\"https://skills.network\" target=\"_blank\">\n","    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\">\n","    </a>\n","</p>\n"]},{"cell_type":"markdown","id":"b550f2f5-75a4-4d33-8f58-036c8a6c0e05","metadata":{},"outputs":[],"source":["# **K Nearest Neighbor**\n"]},{"cell_type":"markdown","id":"3d06954c-3229-4d58-af39-0ffaf58648d2","metadata":{},"outputs":[],"source":["Estimated time needed: **30** minutes\n"]},{"cell_type":"markdown","id":"41d1a3ec-8dcc-4aa7-b031-2e31a27a3f83","metadata":{},"outputs":[],"source":["In this lab, you will learn about and practice the K Nearest Neighbor (KNN) model. KNN is a straightforward but very effective model that can be used for both classification and regression tasks. If the feature space is not very large, KNN can be a high-interpretable model because you can explain and understand how a prediction is made by looking at its nearest neighbors.\n"]},{"cell_type":"markdown","id":"59f2da36-dd98-4c0f-8264-e700d73015ea","metadata":{},"outputs":[],"source":["We will be using a tumor sample dataset containing lab test results about tumor samples. The objective is to classify whether a tumor is malicious (cancer) or benign. As such, it is a typical binary classification task.\n"]},{"cell_type":"markdown","id":"9095f1c6-e34f-406d-9603-d833cff623d9","metadata":{},"outputs":[],"source":["## Objectives\n"]},{"cell_type":"markdown","id":"e7c6c326-9744-4802-a5c4-dfb2d3f98c9d","metadata":{},"outputs":[],"source":["After completing this lab, you will be able to:\n"]},{"cell_type":"markdown","id":"8da1f6ab-df5e-4e6d-94b5-c961adc9d94a","metadata":{},"outputs":[],"source":["* Train KNN models with different neighbor hyper-parameters\n","* Evaluate KNN models on classification tasks\n","* Tune the number of neighbors and find the optimized one for a specific task\n"]},{"cell_type":"markdown","id":"dd60d68e-5e9b-4cc7-b0e7-47811b9bd9ba","metadata":{},"outputs":[],"source":["----\n"]},{"cell_type":"markdown","id":"91bf5a3d-c979-4acc-970c-5e75445c6d60","metadata":{},"outputs":[],"source":["First, let's install `seaborn` for visualization tasks and import required libraries for this lab.\n"]},{"cell_type":"code","id":"86bd13b5-9d3c-4906-aaa6-150f852bc010","metadata":{},"outputs":[],"source":["# All Libraries required for this lab are listed below. The libraries pre-installed on Skills Network Labs are commented.\n# !mamba install -qy pandas==1.3.3 numpy==1.21.2 ipywidgets==7.4.2 scipy==7.4.2 tqdm==4.62.3 matplotlib==3.5.0 seaborn==0.9.0\n# Note: If your environment doesn't support \"!mamba install\", use \"!pip install\"."]},{"cell_type":"code","id":"e91ddc93-2b20-4ecf-abbf-a7423aaa05a3","metadata":{},"outputs":[],"source":["import pandas as pd\nimport numpy as np\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\n# Evaluation metrics related methods\nfrom sklearn.metrics import classification_report, accuracy_score, f1_score, confusion_matrix, precision_recall_fscore_support, precision_score, recall_score\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline"]},{"cell_type":"code","id":"b6234e1a-1f41-4357-bb37-80390dd6f7f4","metadata":{},"outputs":[],"source":["# Define a random seed to reproduce any random process\nrs = 123"]},{"cell_type":"code","id":"b13a430a-c218-49e8-855b-201893eddd1f","metadata":{},"outputs":[],"source":["# Ignore any deprecation warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning) "]},{"cell_type":"markdown","id":"c350ebce-36d5-4967-a970-6961117735b6","metadata":{},"outputs":[],"source":["## Load and explore the tumor sample dataset\n"]},{"cell_type":"markdown","id":"96af8def-225a-48f0-adae-1b23fe0eacc0","metadata":{},"outputs":[],"source":["We first load the dataset `tumor.csv` as a Pandas dataframe:\n"]},{"cell_type":"code","id":"94b43fda-0622-4ee7-ad9d-29ff3039fda3","metadata":{},"outputs":[],"source":["# Read datast in csv format\ndataset_url = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-ML241EN-SkillsNetwork/labs/datasets/tumor.csv\"\ntumor_df = pd.read_csv(dataset_url)"]},{"cell_type":"markdown","id":"1457257b-d522-4d6d-9363-62536b23e288","metadata":{},"outputs":[],"source":["Then, let's quickly take a look at the head of the dataframe.\n"]},{"cell_type":"code","id":"de416508-6b5b-43cc-8d03-117cd9e30338","metadata":{},"outputs":[],"source":["tumor_df.head()"]},{"cell_type":"markdown","id":"30a88eb1-0e0a-4dd5-8134-c3727aae1d56","metadata":{},"outputs":[],"source":["And, display its columns.\n"]},{"cell_type":"code","id":"28c41e8f-bd26-453e-8fca-075010a4dbac","metadata":{},"outputs":[],"source":["tumor_df.columns"]},{"cell_type":"markdown","id":"31ae1a3b-66b1-48a1-b445-431078e4f00f","metadata":{},"outputs":[],"source":["Each observation in this dataset contains lab test results about a tumor sample, such as clump or shapes. Based on these lab test results or features, we want to build a classification model to predict if this tumor sample is malicious (cancer) or benign. The target variable `y` is specified in the `Class` column.\n"]},{"cell_type":"markdown","id":"017ddeeb-6516-4347-b71d-0a5a6e6f7c87","metadata":{},"outputs":[],"source":["Then, let's split the dataset into input `X` and output `y`:\n"]},{"cell_type":"code","id":"1918f87d-bee6-4fa8-95e1-d590d8252352","metadata":{},"outputs":[],"source":["X = tumor_df.iloc[:, :-1]\ny = tumor_df.iloc[:, -1:]"]},{"cell_type":"markdown","id":"499c1f97-2e76-4230-876f-d8be0ebd2b01","metadata":{},"outputs":[],"source":["And, we first check the statistics summary of features in `X`:\n"]},{"cell_type":"code","id":"0cfc3147-865a-4081-bf2b-7da582c845e3","metadata":{},"outputs":[],"source":["X.describe()"]},{"cell_type":"markdown","id":"74151242-d09b-4684-8336-5743a0d81bd9","metadata":{},"outputs":[],"source":["As we can see from the above cell output, all features are numeric and ranged between 1 to 10. This is very convenient as we do not need to scale the feature values as they are already in the same range.\n"]},{"cell_type":"markdown","id":"2fd533f1-80b9-4e10-948f-28e3a5a71801","metadata":{},"outputs":[],"source":["Next, let's check the class distribution of output `y`:\n"]},{"cell_type":"code","id":"0772454c-4719-4a03-9f46-4d95b6f8337c","metadata":{},"outputs":[],"source":["y.value_counts(normalize=True)"]},{"cell_type":"code","id":"6b49c93e-99f3-40e3-8043-6ed8f8e9be52","metadata":{},"outputs":[],"source":["y.value_counts().plot.bar(color=['green', 'red'])"]},{"cell_type":"markdown","id":"e7c99432-5a29-40a2-b401-40074752c33b","metadata":{},"outputs":[],"source":["We have about 65% benign tumors (`Class = 0`) and 35% cancerous tumors (`Class = 1`), which is not a very imbalanced class distribution. \n"]},{"cell_type":"markdown","id":"0a5fdbae-f358-49e5-8668-8458a21b696f","metadata":{},"outputs":[],"source":["## Split training and testing datasets\n"]},{"cell_type":"code","id":"270192fa-cc6a-42a2-9636-a6ba379a58c5","metadata":{},"outputs":[],"source":["# Split 80% as training dataset\n# and 20% as testing dataset\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state = rs)"]},{"cell_type":"markdown","id":"f60b2d01-93a5-4308-a1be-fb645bfcbf51","metadata":{},"outputs":[],"source":["## Train and evaluate a KNN classifier with the number of neighbors set to 2\n"]},{"cell_type":"markdown","id":"4a5a3a2a-b801-4a8c-b972-43f9708a5fd9","metadata":{},"outputs":[],"source":["Training a KNN classifier is very similar to training other classifiers in `sklearn`, we first need to define a `KNeighborsClassifier` object. Here we use `n_neighbors=2` argument to specify how many neighbors will be used for prediction, and we keep other arguments to be their default values.\n"]},{"cell_type":"code","id":"d09d1a98-0abd-4f6c-8a28-78f7de8f4014","metadata":{},"outputs":[],"source":["# Define a KNN classifier with `n_neighbors=2`\nknn_model = KNeighborsClassifier(n_neighbors=2)"]},{"cell_type":"markdown","id":"42b3f8dd-6793-47ac-b75a-34bfb02cfda8","metadata":{},"outputs":[],"source":["Then we can train the model with `X_train` and `y_train`, and we use ravel() method to convert the data frame `y_train` to a vector.\n"]},{"cell_type":"code","id":"5d1ecaeb-a25c-4fb1-84f4-2edc6633594e","metadata":{},"outputs":[],"source":["knn_model.fit(X_train, y_train.values.ravel())"]},{"cell_type":"markdown","id":"4946e670-fda6-44b9-b04f-221e71ea792d","metadata":{},"outputs":[],"source":["And, we can make predictions on the `X_test` dataframe.\n"]},{"cell_type":"code","id":"9cd54089-9581-464f-a3d3-81cd669bd2c9","metadata":{},"outputs":[],"source":["preds = knn_model.predict(X_test)"]},{"cell_type":"markdown","id":"6bcf064e-3dc3-45da-9095-053cf5fc749b","metadata":{},"outputs":[],"source":["To evaluate the KNN classifier, we provide a pre-defined method to return the commonly used evaluation metrics such as accuracy, recall, precision, f1score, and so on, based on the true classes in the 'y_test' and model predictions.\n"]},{"cell_type":"code","id":"01b1a603-b05f-4172-9f69-10e889d4a882","metadata":{},"outputs":[],"source":["def evaluate_metrics(yt, yp):\n    results_pos = {}\n    results_pos['accuracy'] = accuracy_score(yt, yp)\n    precision, recall, f_beta, _ = precision_recall_fscore_support(yt, yp, average='binary')\n    results_pos['recall'] = recall\n    results_pos['precision'] = precision\n    results_pos['f1score'] = f_beta\n    return results_pos"]},{"cell_type":"code","id":"87996eb3-7458-4806-b2a8-e78e5e4578d2","metadata":{},"outputs":[],"source":["evaluate_metrics(y_test, preds)"]},{"cell_type":"markdown","id":"c1b3bfc8-90fd-4243-aa8c-2d4ca835e63c","metadata":{},"outputs":[],"source":["We can see that there is a great classification performance on the tumor sample dataset. This means the KNN model can effectively recognize cancerous tumors.\n","Next, it's your turn to try a different number of neighbors to see if we could get even better performance.\n"]},{"cell_type":"markdown","id":"ec3792f7-f2e2-417a-adc0-f1b2f04923f3","metadata":{},"outputs":[],"source":["## Coding exercise: Train and evaluate a KNN classifier with number of neighbors set to 5\n"]},{"cell_type":"markdown","id":"8e8b49c5-5b47-453c-b5fe-2f23934a4ea3","metadata":{},"outputs":[],"source":["First, define a KNN classifier with KNeighborsClassifier class:\n"]},{"cell_type":"code","id":"63d0b5bb-5e16-4d8b-ad1f-6de63c0857a0","metadata":{},"outputs":[],"source":["# Type your code here\n"]},{"cell_type":"markdown","id":"7ce90789-f7fd-4562-923a-1ef74ac8a07c","metadata":{},"outputs":[],"source":["Then train the model with `X_train` and `y_train`:\n"]},{"cell_type":"code","id":"2e56ba64-a04c-4688-8fe1-ecf5a0555491","metadata":{},"outputs":[],"source":["# Type your code here\n"]},{"cell_type":"markdown","id":"30247e28-d2d2-41af-86a1-e8a5e60a862c","metadata":{},"outputs":[],"source":["And, make predictions on `X_test` dataframe:\n"]},{"cell_type":"code","id":"1c1b1d54-8802-45c8-a207-37a6b7ee4742","metadata":{},"outputs":[],"source":[""]},{"cell_type":"markdown","id":"0552cca1-7058-4eec-9ca7-977fca668bb7","metadata":{},"outputs":[],"source":["At last, you can evaluate your KNN model with provided `evaluate_metrics()` method.\n"]},{"cell_type":"markdown","id":"87af2c5e-d01e-4c60-82db-7f91fbff4321","metadata":{},"outputs":[],"source":["<details><summary>Click here for a sample solution</summary>\n","\n","```python\n","model = KNeighborsClassifier(n_neighbors=5)\n","model.fit(X_train, y_train.values.ravel())\n","preds = model.predict(X_test)\n","evaluate_metrics(y_test, preds)\n","```\n","\n","</details>\n"]},{"cell_type":"markdown","id":"fd614beb-93fd-462f-9050-98fec74847dd","metadata":{},"outputs":[],"source":["## Tune the number of neighbors to find the optmized one\n"]},{"cell_type":"markdown","id":"86ce50ce-fa0e-4061-ab7c-7b416d298710","metadata":{},"outputs":[],"source":["OK, you may wonder which `n_neighbors` argument may give you the best classification performance. We can try different `n_neighbors` (the K value) and check which `K` gives the best classification performance.\n"]},{"cell_type":"markdown","id":"50547286-7aea-434d-b446-856cdcebbb22","metadata":{},"outputs":[],"source":["Here we could try K from 1 to 50, and store the aggregated `f1score` for each k into a list.\n"]},{"cell_type":"code","id":"b50d1e09-fcb0-4347-a62e-8cb149b84f6f","metadata":{},"outputs":[],"source":["# Try K from 1 to 50\nmax_k = 50\n# Create an empty list to store f1score for each k\nf1_scores = []"]},{"cell_type":"markdown","id":"fe92f3e8-5fda-4341-a7f8-7af8af9fed64","metadata":{},"outputs":[],"source":["Then we will train 50 KNN classifiers with K ranged from 1 to 50.\n"]},{"cell_type":"code","id":"9ba338ff-b784-4436-9d84-b87bf77999f7","metadata":{},"outputs":[],"source":["for k in range(1, max_k + 1):\n    # Create a KNN classifier\n    knn = KNeighborsClassifier(n_neighbors=k)\n    # Train the classifier\n    knn = knn.fit(X_train, y_train.values.ravel())\n    preds = knn.predict(X_test)\n    # Evaluate the classifier with f1score\n    f1 = f1_score(preds, y_test)\n    f1_scores.append((k, round(f1_score(y_test, preds), 4)))\n# Convert the f1score list to a dataframe\nf1_results = pd.DataFrame(f1_scores, columns=['K', 'F1 Score'])\nf1_results.set_index('K')"]},{"cell_type":"markdown","id":"5c982089-bddc-48f3-ab67-f5a1771ee8ae","metadata":{},"outputs":[],"source":["This is a long list and different to analysis, so let's visualize the list using a linechart.\n"]},{"cell_type":"code","id":"261c5020-2fde-411f-846c-8bebe4136755","metadata":{},"outputs":[],"source":["# Plot F1 results\nax = f1_results.plot(figsize=(12, 12))\nax.set(xlabel='Num of Neighbors', ylabel='F1 Score')\nax.set_xticks(range(1, max_k, 2));\nplt.ylim((0.85, 1))\nplt.title('KNN F1 Score')"]},{"cell_type":"markdown","id":"d1c9776e-08e5-423d-8e48-a59b1e28b172","metadata":{},"outputs":[],"source":["As we can see from the F1 score linechart, the best `K` value is 5 with about `0.9691` f1score.\n"]},{"cell_type":"markdown","id":"2f88b9bb-ccb5-4289-9898-7d7d236c2f5a","metadata":{},"outputs":[],"source":["## Next steps\n"]},{"cell_type":"markdown","id":"16bedf00-205f-46bf-aac4-893332bbe515","metadata":{},"outputs":[],"source":["Great! Now you have learned about and applied the KNN model to solve a real-world tumor type classification problem. You also tuned the KNN to find the best K value. Later, you will continue learning other popular classification models with different structures, assumptions, cost functions, and application scenarios.\n"]},{"cell_type":"markdown","id":"12139544-40c3-4271-b66f-e2f8766154dc","metadata":{},"outputs":[],"source":["## Authors\n"]},{"cell_type":"markdown","id":"48fd7da4-60d9-4a06-88bc-533186345ae9","metadata":{},"outputs":[],"source":["[Yan Luo](https://www.linkedin.com/in/yan-luo-96288783/)\n"]},{"cell_type":"markdown","id":"d46d827c-e362-4f9c-be90-72b3ce00d089","metadata":{},"outputs":[],"source":["### Other Contributors\n"]},{"cell_type":"markdown","id":"a315b040-ebe9-40f5-8be9-e4ca7df3cbcc","metadata":{},"outputs":[],"source":["## Change Log\n"]},{"cell_type":"markdown","id":"b2a3e564-1d87-4941-b916-2bdbd2f47815","metadata":{},"outputs":[],"source":["|Date (YYYY-MM-DD)|Version|Changed By|Change Description|\n","|-|-|-|-|\n","|2021-11-9|1.0|Yan|Created the initial version|\n","|2022-3-29|1.1|Steve Hord|QA Pass|\n"]},{"cell_type":"markdown","id":"fd7cb51c-d34c-4112-ae01-68a30feddd66","metadata":{},"outputs":[],"source":["Copyright Â© 2021 IBM Corporation. All rights reserved.\n"]}],"metadata":{"kernelspec":{"display_name":"Python","language":"python","name":"conda-env-python-py"},"language_info":{"name":""}},"nbformat":4,"nbformat_minor":4}